\documentclass[]{beamer}
\usepackage{hyperref}

%
\usepackage{listings}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.99,1.0,1.0}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=false,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=0pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


% algo
\usepackage{algorithm,algorithmic}

% math
\usepackage{amsmath}

% tikz
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}

\usetheme[]{Frankfurt}

\title{Differential programming}
\author{Sergey Barseghyan
\href{mailto:barseghyan@phystech.edu}{barseghyan@phystech.edu}}
\date{16.08.2022}


\begin{document}
\begin{frame}
    \maketitle
\end{frame}

\begin{frame}{Outline}
    \tableofcontents
\end{frame}


\section{Non-linear optimization and SABR model}
\subsection{Levenberg-Marquad}
\begin{frame}
    Levenberg-Marquad algorithm - is an optimization algorithm which is basically combination 
    of two objective minimization algorithms: \textbf{Gauss-Newton} and
    \textbf{Gradient Descent} 
    It is very very well suited for the optimization problemsswhere model is \textbf{non-linear} in its parameters 
\end{frame}

\begin{frame}
\frametitle{Non-linear least squares}

    \begin{eqnarray*}
        \hat{y}(t ; \boldsymbol{p}) & \text{- fitted model} \\
        t \in \mathbb{R} & \text{- independent variable} \\
        \left\{t_{i}, y_{i}\right\}^{m}_{i=1} & \text{- observed data points} \\
        W_{ij} & \text{- inverse covariance matrix} \\
        \boldsymbol{p} \in \mathbb{R}^n  & \text{model parameters}
    \end{eqnarray*}
    Optimized objective is nothing else but chi-squared statistic
    \begin{eqnarray*}
        \chi^{2}(\boldsymbol{p})
         =\sum_{i=1}^{m}
         \left[
            \frac{y\left(t_{i}\right)-\hat{y}\left(t_{i} ; \boldsymbol{p}\right)}
                 {\sigma_{y_{i}}}
         \right]^{2} \\
        =(\boldsymbol{y}-\boldsymbol{\hat{y}}(\boldsymbol{p}))^{\top}
         \boldsymbol{W}
         (\boldsymbol{y}-\hat{\boldsymbol{y}}(\boldsymbol{p}))
    \end{eqnarray*}
    
\end{frame}

\begin{frame}
    \frametitle{Gradient Descent and Jacobian update}
    Gradient Descent step update
    \begin{eqnarray*}
        %% Gradient descent
        \frac{\partial}{\partial \boldsymbol{p}} \chi^{2}
         = -2(\boldsymbol{y}-\hat{\boldsymbol{y}})^{\top} \boldsymbol{W} \boldsymbol{J} \\
        %% GD - update
        \boldsymbol{h}_{\mathrm{gd}}=\alpha \boldsymbol{J}^{\top} \boldsymbol{W}(\boldsymbol{y}-\hat{\boldsymbol{y}})
    \end{eqnarray*}
    Gauss-Newton step update
    \begin{eqnarray*}
        %% Gauss-Newton
        \hat{\boldsymbol{y}}(\boldsymbol{p}+\boldsymbol{h})
         \approx 
        \hat{\boldsymbol{y}}(\boldsymbol{p})
        + \left[
            \frac{\partial \hat{\boldsymbol{y}}}{\partial \boldsymbol{p}}
          \right] \boldsymbol{h}
          = \hat{\boldsymbol{y}}+\boldsymbol{J} \boldsymbol{h} \\
        %% Gauss-Newton update
        \frac{\partial}{\partial \boldsymbol{h}}
         \chi^{2}(\boldsymbol{p}+\boldsymbol{h})
         \approx
         -2(\boldsymbol{y}-\hat{\boldsymbol{y}})^{\top}
        \boldsymbol{W} \boldsymbol{J}+2 \boldsymbol{h}^{\top} \boldsymbol{J}^{\top} \boldsymbol{W} \boldsymbol{J} \\
        %%
        \left[
            \boldsymbol{J}^{\top} \boldsymbol{W} \boldsymbol{J}
        \right] \boldsymbol{h}_{\mathrm{gn}}
        =\boldsymbol{J}^{\top} \boldsymbol{W}(\boldsymbol{y}-\hat{\boldsymbol{y}})
    \end{eqnarray*}
\end{frame}

\begin{frame}
    \frametitle{Levenberg-Marquad update}

    \tikzstyle{rect} = [rectangle
                        ,rounded corners
                        ,minimum width=3cm
                        ,minimum height=1cm
                        ,text centered
                        ,draw=black
                        ]

    \tikzstyle{circ} = [circle
                        ,fill=white
                        ,draw=black
                       ]                    

    \tikzstyle{arrow}=[thick,->,>=stealth]                    
    \begin{tikzpicture}[]
        % Nodes
        \node[rect] (algo1) at (0, 4){Gradient Descent};
        \node[rect] (algo2) at (8, 4) {Gauss Newton};
        \node[rect] (algo3) at (4, 2.5) {Levenberg-Marquad};
        \node[circ] (plus)  at (4, 4){+};

        % Draw
        \draw (algo1) edge (plus);
        \draw (plus) edge (algo2);
        \draw[arrow] (plus) -- (algo3);
    \end{tikzpicture}   
    
    \begin{eqnarray*}
        \left[\boldsymbol{J}^{\top} \boldsymbol{W} \boldsymbol{J}
              +\lambda \operatorname{diag}\left(\boldsymbol{J}^{\top} \boldsymbol{W} \boldsymbol{J}\right)
        \right]
         \boldsymbol{h}_{\mathrm{lm}}
         =\boldsymbol{J}^{\top} \boldsymbol{W}(\boldsymbol{y}-\hat{\boldsymbol{y}})
    \end{eqnarray*}

    \begin{eqnarray*}
        \lambda \text{ is dumping parameter}
    \end{eqnarray*}
    
\end{frame}


\begin{frame}
    
\begin{algorithm}[H]
    \caption{LM algorithm parameter update}
\begin{algorithmic}[1]
    \STATE $\lambda_{0} = \mathrm{\lambda}_{o}$ \\
    Calculate $\boldsymbol{h_{lm}}$ and $\rho$
    \IF{$\rho_{i}(\boldsymbol{h_{lm}}) > \epsilon_{4}$}
        \STATE $\boldsymbol{p} \leftarrow \boldsymbol{p} + \boldsymbol{h_{lm}}$;
        \STATE $\lambda_{i+1} = \max\left[\lambda_{i}/L_{down}, 10^{-7}\right]$
    \ELSE
        \STATE $\lambda_{i+1} = \min\left[\lambda_{i} L_{up}, 10^{7}\right]$
    \ENDIF
\label{alg:seq}
\end{algorithmic}
\end{algorithm}

Where
\begin{equation*}
\rho_{i}(\boldsymbol{h_{lm}}) = \frac{\chi^{2}(\boldsymbol{p})-\chi^{2}\left(\boldsymbol{p}+\boldsymbol{h}_{\mathrm{lm}}\right)}
      {\boldsymbol{h}_{\operatorname{lm}}^{\top}\left(\lambda_{i} \operatorname{diag}\left(\boldsymbol{J}^{\top} \boldsymbol{W} J\right) \boldsymbol{h}_{\mathrm{lm}}
      +\boldsymbol{J}^{\top} \boldsymbol{W}(\boldsymbol{y}
      -\hat{\boldsymbol{y}}(\boldsymbol{p}))\right)}
\end{equation*}

   
\end{frame}

\begin{frame}
    \frametitle{Jacobian updade}

    For every $2n$ interation where $\chi^{2}(\boldsymbol{p}+\boldsymbol{h})>\chi^{2}(\boldsymbol{p})$
    forward or central difference is calculated for Jacobian
    \begin{eqnarray*}
        J_{i j}
        =\frac{\partial \hat{y}_{i}}{\partial p_{j}}
            =\frac{\hat{y}\left(t_{i} ; \boldsymbol{p}+\delta \boldsymbol{p}_{j}\right)-\hat{y}\left(t_{i} ; \boldsymbol{p}\right)}
            {\left\|\delta \boldsymbol{p}_{j}\right\|} \\
        %%%
        J_{i j}=\frac{\partial \hat{y}_{i}}{\partial p_{j}}
        =\frac{\hat{y}\left(t_{i} ; \boldsymbol{p}+\delta \boldsymbol{p}_{j}\right)-\hat{y}\left(t_{i} ; \boldsymbol{p}-\delta \boldsymbol{p}_{j}\right)}
        {2\left\|\delta \boldsymbol{p}_{j}\right\|}
    \end{eqnarray*}
    For intermediate iterations Broyden's rank-1 update formula is used in order not to make expensive calculations
    on every interation.
    \begin{equation*}
        \boldsymbol{J^{i+1}}=\boldsymbol{J^{i}}+\frac{(\hat{\boldsymbol{y}}(\boldsymbol{p}
                                                                        +\boldsymbol{h})
                                    -\hat{\boldsymbol{y}}(\boldsymbol{p})-\boldsymbol{J^{i}} \boldsymbol{h}) \boldsymbol{h}^{\top}}
                            {\boldsymbol{h}^{\top} \boldsymbol{h}}
    \end{equation*}
\end{frame}


\begin{frame}[fragile]
    \begin{lstlisting}[language=Python]
    @torch.no_grad()
    def broyden_jacobian_update(self):
        '''
        Broyden 1-rank Jacobian update
        '''
        df = self.func(self.p + self.dp) - self.func(self.p)
        self.J += torch.outer(df - torch.mv(self.J, self.dp),
                                self.dp) \
                            .div(torch.linalg.norm(self.dp, ord=2))
    @torch.no_grad()    
    def torch_jacobian_update(self, p):
        '''
        Finite-difference Jacobian update
        '''
        self.J = torch.autograd.functional.jacobian(self.func, p)
        \end{lstlisting}
\end{frame}


\begin{frame}[fragile]
    \begin{lstlisting}[language=Python]
    @torch.no_grad()        
    def solve_for_dp(self):
        '''
        Solver for optimizer step
        '''
        self.JTW = torch.matmul(torch.transpose(self.J, 0, 1),
                                    self.W)
        self.JTWJ =  torch.matmul(self.JTW, self.J)
        
        dy =  self.y_data - self.func(self.p)
        self.dp = torch.linalg.solve(self.JTWJ 
                                    + self.lambda_lm 
                                    * torch.diag(
                                        torch.diagonal(self.JTWJ)
                                                ), 
                                    torch.mv(self.JTW , dy)
                                    )
    
    \end{lstlisting}
\end{frame}





\subsection{SABR model}

\begin{frame}
    \frametitle{Motivation for the model}
\end{frame}

\begin{frame}
    \frametitle{Model Definition}
\end{frame}


\begin{frame}
    \frametitle{Exact solution}
\end{frame}



\subsection{}

\section{Diffential programming for detectors}
\begin{frame}
   
\end{frame}

\end{document}
